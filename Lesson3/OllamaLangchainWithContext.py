# Import OllamaLLM from the updated langchain_ollama package
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

template = """
Answer the question below.
Here is the conversation history: {context}
Question: {question}
Answer:
"""

# Initialize the Ollama LLM with the specified model (e.g., "llama2")
# Ensure that Ollama is running and the 'llama2' model is available on your system.
llm = OllamaLLM(model="mistral")
prompt = ChatPromptTemplate.from_template(template=template)

chain = prompt | llm

def handle_chat():
    context = ""
    print("Welcome AI chatbot. type 'exit' to quit")
    while True:
        user_input = input("Prompt: ")
        if user_input.lower() == 'exit':
            break
        # Invoke the LLM with a prompt to tell a joke
        result = chain.invoke({"context": f"{context}", "question": f"{user_input}"})
        # Print the result generated by the LLM
        print(result)
        # Save History
        context += f"\nuser : {user_input} \n AI:{result}"


handle_chat()
